---
title: 02. Information Theory
author: Tao He
date: 2019-04-29
category: Jekyll
layout: post
---

## 2.1 Entropy

long contents .....

1. a
2. b
3. c
4. d

## 2.2 Properties of Entropy

long contents .....

- 1
- 2
- 3
- 4

## 2.3 Cross Entropy Loss

long contents .....

1. e
2. f
3. g
4. h

## 2.4 Jointly Distributed Random Variables

### 2.4.1 Joint Entropy

### 2.4.2 Conditional Entropy

### 2.4.3 Mutual Information

### 2.4.4 Properties of Mutual Information

### 2.4.5 Conditional Mutual Information

## 2.5 Random Process

**확률 과정(Random Process)이란?**

확률 과정은 다음과 같이 정의됩니다:

$$
X = \{X_n\}_{n=1}^{\infty}
$$

이것은 순서를 가진 확률 변수들의 집합이며, 각 $X_n$은 특정 확률 분포를 따릅니다.

- 여기서 $n$은 꼭 시간(time)을 의미하지 않아도 됩니다.
- 예시:
  - 텍스트: $X_1$은 첫 번째 문자, $X_2$는 두 번째 문자 등
  - 이미지: $X_{i,j}$는 $i$행 $j$열 픽셀의 밝기
  - 시계열: $X_t$는 $t$초 후의 값

**핵심**: 인덱스가 시간일 필요는 없으며, **순서만 있으면 확률 과정**이 됩니다.

---

**i.i.d. 과정이란?**

i.i.d.는 independent and identically distributed의 약자입니다.

- **독립(independent)**: 각 $X_n$이 서로 영향을 주지 않음
- **동일 분포(identically distributed)**: 모든 $X_n$이 동일한 확률 분포를 따름

전체 확률은 다음처럼 단순 곱으로 계산할 수 있습니다:

$$
P(X_1 = x_1, \dots, X_n = x_n) = \prod_{i=1}^{n} P(X_i = x_i)
$$

---

**현실은 대부분 i.i.d.가 아님**

예: 텍스트

- "progra_ing"이라는 단어에서 빈칸에 'm'이 올 가능성이 높다고 판단할 수 있음
- 이는 앞뒤 문맥이 영향을 주기 때문 → 요소들 간에 **의존성 존재**

**결론**: 현실의 데이터는 보통 독립적이지 않고, 앞뒤 요소에 영향을 받습니다.

i.i.d.가 아닌 경우 사용하는 모델들:

- 마르코프 모델 (Markov Model)
- 순환 신경망 (Recurrent Neural Network, RNN)
- 트랜스포머 (Transformer)

### 2.5.1 What is Markovian?

### 2.5.2 1st Order Markov Process

### 2.5.3 kth Order Markov Process

### 2.5.4 Stationary Distribution

### 2.5.5 Stationary Markov Process

## 2.6 Continuous Random Variables

### 2.6.1 Probability Density Function

### 2.6.2 Gaussian

### 2.6.3 Differential Entropy

### 2.6.4 Properties of Differential Entropy

### 2.6.5 Joint Differential Entropy

### 2.6.6 Maximum Differential Entropy
