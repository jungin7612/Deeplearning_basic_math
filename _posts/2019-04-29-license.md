---
title: 02. Information Theory
author: Tao He
date: 2019-04-29
category: Jekyll
layout: post
---

## 2.1 Entropy

long contents .....

1. a
2. b
3. c
4. d

## 2.2 Properties of Entropy

long contents .....

- 1
- 2
- 3
- 4

## 2.3 Cross Entropy Loss

long contents .....

1. e
2. f
3. g
4. h

## 2.4 Jointly Distributed Random Variables

### 2.4.1 Joint Entropy

### 2.4.2 Conditional Entropy

### 2.4.3 Mutual Information

### 2.4.4 Properties of Mutual Information

### 2.4.5 Conditional Mutual Information

## 2.5 Random Process

### 2.5.1 What is Markovian?

### 2.5.2 1st Order Markov Process

### 2.5.3 kth Order Markov Process

### 2.5.4 Stationary Distribution

### 2.5.5 Stationary Markov Process

## 2.6 Continuous Random Variables

### 2.6.1 Probability Density Function

### 2.6.2 Gaussian

### 2.6.3 Differential Entropy

### 2.6.4 Properties of Differential Entropy

### 2.6.5 Joint Differential Entropy

### 2.6.6 Maximum Differential Entropy
